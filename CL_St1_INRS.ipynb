{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "810d9f10-ec19-4b09-8f90-e983e460b319",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://laelgelcpublic.s3.sa-east-1.amazonaws.com/lael_50_years_narrow_white.png.no_years.400px_96dpi.png\" width=\"300\" alt=\"LAEL 50 years logo\">\n",
    "<h3>APPLIED LINGUISTICS GRADUATE PROGRAMME (LAEL)</h3>\n",
    "</center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c2c96-2fc3-4a1a-995b-c388036a2a15",
   "metadata": {},
   "source": [
    "# Corpus Linguistics - Study 1 - INRS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9c2af7-9fc1-4f51-a4f5-2ed915b93039",
   "metadata": {},
   "source": [
    "## FakeNewsCorpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b1a09b-98b9-4c78-9617-cd6829d2cfc7",
   "metadata": {},
   "source": [
    "[FakeNewsCorpus](https://github.com/several27/FakeNewsCorpus) is mainly intended for use in training deep learning algorithms for purpose of fake news recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454ccb1c-1dc7-44cf-8120-a433cbc19915",
   "metadata": {},
   "source": [
    "## Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eade79cc-3483-441c-9dca-a342df32a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34e5dfa-bdad-47f4-8a29-7210c2b565d4",
   "metadata": {},
   "source": [
    "## Sampling the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1558ccb-d87c-47f5-bcef-5b30eba6c35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the data set (216,212,648 rows)\n",
    "data_set = 'news_cleaned_2018_02_13.csv'\n",
    "\n",
    "# Defining the number of rows to be sampled\n",
    "number_of_rows = 100\n",
    "\n",
    "# Creating a directory to store output files\n",
    "output_dir = 'fakenewscorpus_sample'\n",
    "output_file_key = 'news_cleaned_sample' # The output file will have this key plus a suffix\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Explicitly defining data types for all columns as 'str'\n",
    "column_data_types = {\n",
    "    'id': str,\n",
    "    'domain': str,\n",
    "    'type': str,\n",
    "    'url': str,\n",
    "    'content': str,\n",
    "    'scraped_at': str,\n",
    "    'inserted_at': str,\n",
    "    'updated_at': str,\n",
    "    'title': str,\n",
    "    'authors': str,\n",
    "    'keywords': str,\n",
    "    'meta_keywords': str,\n",
    "    'meta_description': str,\n",
    "    'tags': str,\n",
    "    'summary': str,\n",
    "    'source': str\n",
    "}\n",
    "\n",
    "# Use the 'read_csv' function to sample\n",
    "df_sample = pd.read_csv(data_set, encoding='utf-8', nrows=number_of_rows, dtype=column_data_types)\n",
    "i = 0\n",
    "filename = f'{output_dir}/{output_file_key}{i+1:02d}.csv'\n",
    "df_sample.to_csv(filename, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46be542b-a25c-4c44-96d6-f1ba553c7a37",
   "metadata": {},
   "source": [
    "## Partitioning the data set into smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12478ef-4a75-418b-a9dd-f18276941c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eyamr\\AppData\\Local\\Temp\\ipykernel_16660\\3168893946.py:36: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-21 17:36:17 : Processing fakenewscorpus/news_cleaned01.csv\n",
      "2024-06-21 17:36:49 : Processed fakenewscorpus/news_cleaned01.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eyamr\\AppData\\Local\\Temp\\ipykernel_16660\\3168893946.py:36: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-21 17:37:03 : Processing fakenewscorpus/news_cleaned02.csv\n",
      "2024-06-21 17:37:36 : Processed fakenewscorpus/news_cleaned02.csv\n"
     ]
    }
   ],
   "source": [
    "# Defining the data set (216,212,648 rows)\n",
    "data_set = 'news_cleaned_2018_02_13.csv'\n",
    "\n",
    "# Defining the chunk size\n",
    "chunk_size = 300000  # This will create chunks of 500,000 rows, enabling the data set to be partitioned into approximately 30 chunks\n",
    "\n",
    "# Creating a directory to store output files\n",
    "output_dir = 'fakenewscorpus'\n",
    "output_file_key = 'news_cleaned' # The output files will have the same key plus a suffix\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Explicitly defining data types for all columns as 'str'\n",
    "column_data_types = {\n",
    "    'id': str,\n",
    "    'domain': str,\n",
    "    'type': str,\n",
    "    'url': str,\n",
    "    'content': str,\n",
    "    'scraped_at': str,\n",
    "    'inserted_at': str,\n",
    "    'updated_at': str,\n",
    "    'title': str,\n",
    "    'authors': str,\n",
    "    'keywords': str,\n",
    "    'meta_keywords': str,\n",
    "    'meta_description': str,\n",
    "    'tags': str,\n",
    "    'summary': str,\n",
    "    'source': str\n",
    "}\n",
    "\n",
    "# Use the 'read_csv' function with 'iterator' and 'chuncksize' parameters\n",
    "reader = pd.read_csv(data_set, encoding='utf-8', iterator=True, chunksize=chunk_size, dtype=column_data_types)\n",
    "\n",
    "i = 0\n",
    "for chunk in reader:\n",
    "    filename = f'{output_dir}/{output_file_key}{i+1:02d}.csv'\n",
    "    timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(timestamp, ': Processing ' + filename)\n",
    "    chunk.to_csv(filename, index=False)\n",
    "    timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(timestamp, ': Processed ' + filename)\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437f2a3a-b9b5-4ffe-a8a5-982df993adbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
